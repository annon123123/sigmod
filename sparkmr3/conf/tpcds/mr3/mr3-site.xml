<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>

<property>
  <name>mr3.runtime</name>
  <value>tez</value>
</property>

<property>
  <name>mr3.master.mode</name>
  <value>yarn</value>
  <description>
    local-thread: run DAGAppMaster in the same process of MR3Client in client side
    local-process: run DAGAppMaster in a separate process in client side
    yarn: run DAGAppMaster in a container in cluster side
  </description>
</property>

<property>
  <name>mr3.am.worker.mode</name>
  <value>yarn</value>
</property>

<property>
  <name>mr3.am.resource.memory.mb</name>
  <value>${mr3.am.heapsize}</value>
</property>

<property>
  <name>mr3.am.resource.cpu.cores</name>
  <value>1</value>
</property>

<property>
  <name>mr3.am.local.resourcescheduler.max.memory.mb</name>
  <value>8192</value>
</property>

<property>
  <name>mr3.am.local.resourcescheduler.max.cpu.cores</name>
  <value>8</value>
</property>

<property>
  <name>mr3.cluster.use.hadoop-libs</name>
  <value>false</value>
</property>

<property>
  <name>mr3.lib.uris</name>
  <value>${liburis}</value>
</property>

<property>
  <name>mr3.aux.uris</name>
  <value>${auxuris}</value>
  <!-- <value>${auxuris},/user/hive/lib/mr3-truststore.jks,/user/hive/lib/mr3-keystore.jks</value> -->
</property>

<property>
  <name>mr3.cluster.additional.classpath</name>
  <value>${addclasspath}:${amprocess.classpath}</value>
</property>

<property>
  <name>mr3.am.launch.cmd-opts</name>
  <value>-server -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -XX:+UseNUMA -XX:+UseG1GC -XX:+ResizeTLAB</value>
  <!-- <value>${am.honestopts} ${am.jfropts1} ${am.jfropts2} ${am.jfropts3} ${am.jfropts4} ${am.jfropts5} ${am.jfropts6} ${am.yourkitopts} -server -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -XX:+PrintGCDetails -verbose:gc -XX:+PrintGCTimeStamps -XX:+UseNUMA -XX:+UseG1GC -XX:+ResizeTLAB</value> -->
</property>

<property>
  <name>mr3.am.staging-dir</name>
  <value>/tmp/${user.name}/staging</value>
</property>

<property>
  <name>mr3.am.local.working-dir</name>
  <value>/tmp/${user.name}/working-dir</value>
</property>

<property>
  <name>mr3.am.local.log-dir</name>
  <value>/tmp/${user.name}/log-dir</value>
</property>

<property>
  <name>mr3.am.log.level</name>
  <value>INFO</value>
</property>

<property>
  <name>mr3.am.generate.dag.graph.viz</name>
  <value>false</value>
</property>

<property>
  <name>mr3.container.kill.policy</name>
  <value>container.kill.wait.workervertex</value>
  <description>
    container.kill.wait.workervertex: wait until WorkerVertexes terminate
    container.kill.nowait: kill without waiting 
  </description>
</property>

<property>
  <name>mr3.am.max.num.concurrent.dags</name>
  <value>128</value>
</property>

<property>
  <name>mr3.dag.priority.scheme</name>
  <value>fifo</value>
</property>
 
<property>
  <name>mr3.am.task.max.failed.attempts</name>
  <value>3</value>
</property>

<property>
  <name>mr3.am.task.retry.on.fatal.error</name>
  <value>true</value>
</property>

<property>
  <name>mr3.am.task.no.retry.errors</name>
  <value>MapJoinMemoryExhaustionError,OutOfMemoryError</value>
</property>

<property>
  <name>mr3.am.client.thread-count</name>
  <value>32</value>
</property>

<property>
  <name>mr3.async.logging</name>
  <value>true</value>
</property>

<property>
  <name>mr3.am.permit.custom.user.class</name>
  <value>false</value>
</property>

<!-- resource scheduler -->

<property>
  <name>mr3.am.resourcescheduler.max.requests.per.taskscheduler</name>
  <value>1000</value>
</property>

<!-- container -->

<property>
  <name>mr3.container.resourcescheduler.type</name>
  <value>yarn</value>
</property>

<property>
  <name>mr3.container.launch.cmd-opts</name>
  <value>-XX:+AlwaysPreTouch -Xss512k -XX:+UseG1GC -XX:TLABSize=8m -XX:+ResizeTLAB -XX:+UseNUMA -XX:+AggressiveOpts -XX:InitiatingHeapOccupancyPercent=40 -XX:G1ReservePercent=20 -XX:MaxGCPauseMillis=200 -XX:MetaspaceSize=1024m -server -Djava.net.preferIPv4Stack=true -XX:NewRatio=8 -XX:+UseNUMA </value>
  <!-- <value>${container.honestopts} ${container.jfropts1} ${container.jfropts2} ${container.jfropts3} ${container.jfropts4} ${container.jfropts5} ${container.jfropts6} ${container.yourkitopts} -XX:+AlwaysPreTouch -Xss512k -XX:+UseG1GC -XX:TLABSize=8m -XX:+ResizeTLAB -XX:+UseNUMA -XX:+AggressiveOpts -XX:InitiatingHeapOccupancyPercent=40 -XX:G1ReservePercent=20 -XX:MaxGCPauseMillis=200 -XX:MetaspaceSize=1024m -server -Djava.net.preferIPv4Stack=true -XX:NewRatio=8 -XX:+UseNUMA -XX:+PrintGCDetails -verbose:gc -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=4 -XX:GCLogFileSize=100M -XX:+PrintGCDateStamps -XX:SoftRefLRUPolicyMSPerMB=25</value> -->
</property>

<property>
  <name>mr3.container.reuse</name>
  <value>true</value>
</property>

<property>
  <name>mr3.container.stop.cross.dag.reuse</name>
  <value>false</value>
</property>

<property>
  <name>mr3.container.log.level</name>
  <value>INFO</value>
</property>

<property>
  <name>mr3.container.idle.timeout.ms</name>
  <value>3600000</value>
</property>

<property>
  <name>mr3.heartbeat.task.timeout.ms</name>
  <value>120000</value>
</property>

<property>
  <name>mr3.heartbeat.container.timeout.ms</name>
  <value>600000</value>
</property>

<property>
  <name>mr3.app.history.logging.enabled</name>
  <value>false</value>
</property>

<property>
  <name>mr3.dag.history.logging.enabled</name>
  <value>false</value>
</property>

<property>
  <name>mr3.task.history.logging.enabled</name>
  <value>false</value>
</property>

<property>
  <name>mr3.am.node-blacklisting.enabled</name>
  <value>false</value>
</property>

<property>
  <name>mr3.am.maxtaskfailure.percent</name>
  <value>1</value>
</property>

<property>
  <name>mr3.container.termination.checker.timeout.ms</name>
  <value>300000</value>
</property>

<property>
  <name>mr3.container.task.failure.num.sleeps</name>
  <value>0</value>
</property>

<!-- auto-scaling -->

<property>
  <name>mr3.enable.auto.scaling</name>
  <value>false</value>
</property>

<property>
  <name>mr3.memory.usage.check.scheme</name>
  <value>average</value>
</property>

<property>
  <name>mr3.auto.scale.out.threshold.percent</name>
  <value>80</value>
</property>

<property>
  <name>mr3.auto.scale.in.threshold.percent</name>
  <value>50</value>
</property>

<property>
  <name>mr3.memory.usage.check.window.length.secs</name>
  <value>600</value>
</property>

<property>
  <name>mr3.check.memory.usage.event.interval.secs</name>
  <value>10</value>
</property>

<property>
  <name>mr3.auto.scale.out.grace.period.secs</name>
  <value>300</value>
</property>

<property>
  <name>mr3.auto.scale.in.delay.after.scale.out.secs</name>
  <value>60</value>
</property>

<property>
  <name>mr3.auto.scale.in.grace.period.secs</name>
  <value>300</value>
</property>

<property>
  <name>mr3.auto.scale.in.wait.dag.finished</name>
  <value>true</value>
</property>

<property>
  <name>mr3.auto.scale.out.num.initial.containers</name>
  <value>4</value>
</property>

<property>
  <name>mr3.auto.scale.out.num.increment.containers</name>
  <value>1</value>
</property>

<property>
  <name>mr3.auto.scale.in.num.decrement.hosts</name>
  <value>1</value>
</property>

<property>
  <name>mr3.auto.scale.in.min.hosts</name>
  <value>1</value>
</property>

<property>
  <name>mr3.am.task.concurrent.run.threshold.percent</name>
  <value>99</value>
</property>

<property>
  <name>mr3.am.task.concurrent.run.enable.root.vertex</name>
  <value>true</value>
</property>

<!-- Prometheus -->

<property>
  <name>mr3.prometheus.enable.metrics</name>
  <value>true</value>
</property>

<property>
  <name>mr3.prometheus.enable.jvm.metrics</name>
  <value>true</value>
</property>

<property>
  <name>mr3.k8s.master.pod.additional.labels</name>
  <value>hivemr3_aux=prometheus</value>
</property>

<property>
  <name>mr3.prometheus.worker.enable.metrics</name>
  <value>false</value>
</property>

<property>
  <name>mr3.prometheus.worker.enable.jvm.metrics</name>
  <value>true</value>
</property>

<property>
  <name>mr3.prometheus.worker.httpserver.port</name>
  <value>0</value>
</property>

<!-- for Spark on MR3 -->

<property>
  <name>mr3.container.combine.taskattempts</name>
  <value>true</value>
</property>

<property>
  <name>mr3.container.mix.taskattempts</name>
  <value>true</value>
</property>

<property>
  <name>mr3.container.scheduler.scheme</name>
  <value>none</value>
</property>

</configuration>
